{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f085970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Device: NVIDIA RTX A1000 Laptop GPU\n",
      "Allocated: 0.00 MB\n",
      "Reserved: 0.00 MB\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "# 1. Get the name of the GPU\n",
    "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 2. Memory currently occupied by Tensors\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# 3. Memory reserved by PyTorch caching allocator (Total memory taken from OS)\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# 4. To see a full summary report\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a1479",
   "metadata": {},
   "source": [
    "# OpenAI Whisper-large-v3-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float16\n",
    "\n",
    "# model_id = \"openai/whisper-large-v3-turbo\"\n",
    "model_id = \"openai/whisper-tiny\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=1,\n",
    "    dtype=torch_dtype,\n",
    "    device=device,\n",
    "    generate_kwargs={\"language\": \"korean\", \"task\": \"transcribe\"},\n",
    "    return_timestamps=True,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6642320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import re\n",
    "def inference_turbo(path: str):\n",
    "    audio, sr = librosa.load(path, sr=16000)\n",
    "    result = pipe(audio)\n",
    "    text_clean = re.sub(r'\\.', '', result['text'])\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26976b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inference_turbo(\"data/scope_phoneme_data/A long/A long 1.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4860d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"data/scope_phoneme_data\"\n",
    "data_records = []\n",
    "\n",
    "for folder_name in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".wav\"):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                phonemes = inference(file_path)\n",
    "                \n",
    "                data_records.append({\n",
    "                    \"Folder/Label\": folder_name,\n",
    "                    \"File Name\": file_name,\n",
    "                    \"Predicted Phonemes\": phonemes\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(data_records)\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"phoneme_whisper_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4f0ec",
   "metadata": {},
   "source": [
    "# OpenAI Whisper-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2f068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 167/167 [00:00<00:00, 414.10it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.weight]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (93680,) 16000\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "print(type(sample[\"array\"]), sample[\"array\"].shape, sample[\"sampling_rate\"])\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features)\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99c6874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "def inference_tiny(path: str):\n",
    "    audio, sr = librosa.load(path, sr=16000)\n",
    "    input_features = processor(audio, sampling_rate=sr, return_tensors=\"pt\").input_features \n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    return transcription[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad0258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Folder/Label File Name Predicted Phonemes\n",
      "0            B   B 1.wav                 끝!\n",
      "1            B   B 4.wav               Bye!\n",
      "2            B   B 2.wav                 안녕\n",
      "3            B   B 3.wav               Bye!\n",
      "4            Q   Q 1.wav               그...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"data/scope_phoneme_data\"\n",
    "data_records = []\n",
    "\n",
    "for folder_name in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".wav\"):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                phonemes = inference_tiny(file_path)\n",
    "                \n",
    "                data_records.append({\n",
    "                    \"Folder/Label\": folder_name,\n",
    "                    \"File Name\": file_name,\n",
    "                    \"Predicted Phonemes\": phonemes\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(data_records)\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"phoneme_whisper_tiny_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
