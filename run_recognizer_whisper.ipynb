{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f085970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "# 1. Get the name of the GPU\n",
    "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 2. Memory currently occupied by Tensors\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# 3. Memory reserved by PyTorch caching allocator (Total memory taken from OS)\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# 4. To see a full summary report\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a1479",
   "metadata": {},
   "source": [
    "# OpenAI Whisper-large-v3-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float16\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=1,\n",
    "    dtype=torch_dtype,\n",
    "    device=device,\n",
    "    generate_kwargs={\"language\": \"english\", \"task\": \"transcribe\"},\n",
    "    return_timestamps=True,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6642320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import re\n",
    "def inference_turbo(path: str):\n",
    "    audio, sr = librosa.load(path, sr=16000)\n",
    "    with torch.no_grad():\n",
    "        result = pipe(audio)\n",
    "    text_clean = re.sub(r'\\.', '', result['text'])\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b631bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inference_turbo(\"data/scope_phoneme_data/A long/A long 1.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4860d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "data_path = \"data/scope_phoneme_data\"\n",
    "data_records = []\n",
    "\n",
    "for folder_name in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".wav\"):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                phonemes = inference_turbo(file_path)\n",
    "                \n",
    "                data_records.append({\n",
    "                    \"Folder/Label\": folder_name,\n",
    "                    \"File Name\": file_name,\n",
    "                    \"Predicted Phonemes\": phonemes\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(data_records)\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"results/phoneme_whisper_turbo32_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4f0ec",
   "metadata": {},
   "source": [
    "# OpenAI Whisper-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a79b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model and processor\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer, # type: ignore\n",
    "    feature_extractor=processor.feature_extractor, # type: ignore\n",
    "    chunk_length_s=30,\n",
    "    batch_size=1,\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    "    generate_kwargs={\"language\": \"english\", \"task\": \"transcribe\"},\n",
    "    return_timestamps=True,\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n",
    "\n",
    "# # load dummy dataset and read audio files\n",
    "# ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "# sample = ds[0][\"audio\"]\n",
    "# print(type(sample[\"array\"]), sample[\"array\"].shape, sample[\"sampling_rate\"])\n",
    "# input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n",
    "\n",
    "# # generate token ids\n",
    "# predicted_ids = model.generate(input_features)\n",
    "# transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "def inference_tiny(path: str):\n",
    "    audio, sr = librosa.load(path, sr=16000)\n",
    "    input_features = processor(audio, sampling_rate=sr, return_tensors=\"pt\").input_features .to(device)\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    return transcription[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"data/scope_phoneme_data\"\n",
    "data_records = []\n",
    "\n",
    "for folder_name in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.endswith(\".wav\"):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                phonemes = inference_tiny(file_path)\n",
    "                \n",
    "                data_records.append({\n",
    "                    \"Folder/Label\": folder_name,\n",
    "                    \"File Name\": file_name,\n",
    "                    \"Predicted Phonemes\": phonemes\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(data_records)\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"results/phoneme_whisper_tiny_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
